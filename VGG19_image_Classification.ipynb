{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81ps5LMLbSaV",
    "outputId": "888a4c83-8ecd-4395-ade5-fe5e0a07616a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4996ee3d8d09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/gdrive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UzwwmwACbc6L",
    "outputId": "573026c2-9096-4a4f-e24e-5fe4e010961d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: \"'/content/gdrive/My Drive/Gdrive'\"\n",
      "D:\\Python_PhD_Project\\Image-Classification-using-VGG19-and-Resnet-main\n"
     ]
    }
   ],
   "source": [
    "cd '/content/gdrive/My Drive/Gdrive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXFVqQisgPIW",
    "outputId": "3ed8e45f-06a1-4dcc-e30b-8a69d6e6675a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is 8213-68E9\n",
      "\n",
      " Directory of D:\\Python_PhD_Project\\Image-Classification-using-VGG19-and-Resnet-main\n",
      "\n",
      "11/10/2021  10:59 AM    <DIR>          .\n",
      "11/10/2021  10:59 AM    <DIR>          ..\n",
      "08/10/2021  11:22 PM    <DIR>          .idea\n",
      "11/10/2021  10:59 AM    <DIR>          .ipynb_checkpoints\n",
      "08/10/2021  11:18 PM    <DIR>          assets\n",
      "08/10/2021  11:18 PM    <DIR>          gambar\n",
      "08/10/2021  11:18 PM    <DIR>          models\n",
      "11/12/2020  02:24 PM             2,037 README.md\n",
      "11/12/2020  02:24 PM           235,707 resnet50_image_classifier.ipynb\n",
      "11/12/2020  02:24 PM           856,053 testing_load_image.ipynb\n",
      "11/10/2021  10:59 AM            24,282 VGG19_image_Classification.ipynb\n",
      "               4 File(s)      1,118,079 bytes\n",
      "               7 Dir(s)  136,835,428,352 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdlRvgSNBjEr",
    "outputId": "69a474e6-27b7-4272-d097-f5c818c3093a"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0m_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\imp.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, file, filename, details)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[1;34m(name, path, file)\u001b[0m\n\u001b[0;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 74\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\bhuiy\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\bhuiy\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\bhuiy\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\Users\\bhuiy\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Users\\bhuiy\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-88aaa5354a82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     raise ImportError(\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import itertools\n",
    "import keras\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img \n",
    "from keras.models import Sequential \n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Dropout, Flatten, Dense  \n",
    "from keras import applications  \n",
    "from keras.utils.np_utils import to_categorical  \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "import math  \n",
    "import datetime\n",
    "import time\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "print('Tensorflow_VER= V',tf.version.VERSION)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HU3cqdCdh_Qm",
    "outputId": "95ff763f-dc24-4049-c345-0c66770ef047"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'applications' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-91350ac5bb19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mvgg19\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVGG19\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mvgg19\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Arsitekturnya Wan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'applications' is not defined"
     ]
    }
   ],
   "source": [
    "#Feature Extraction cuy\n",
    "img_width, img_height = 224, 224  \n",
    "\n",
    "train_data_dir = '/content/gdrive/My Drive/Gdrive/train/'  \n",
    "test_data_dir = '/content/gdrive/My Drive/Gdrive/test/'\n",
    "validation_data_dir = '/content/gdrive/My Drive/Gdrive/val/'\n",
    "\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model_vgg19.h5' \n",
    "\n",
    "\n",
    "#TUNING SEBAGIAN DISINI\n",
    "batch_size = 8\n",
    "lr=1e-4\n",
    "opt='rmsprop'\n",
    "epochs = 100 \n",
    "  \n",
    "\n",
    "vgg19 = applications.VGG19(include_top=False, weights='imagenet') \n",
    "vgg19.summary() #Arsitekturnya Wan\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1. / 255)  \n",
    "train_datagen = ImageDataGenerator(#rescale=1. / 255) \n",
    "        rescale=1. / 255,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True)    \n",
    "#\n",
    "\n",
    "#Pre train Training\n",
    "start = datetime.datetime.now()\n",
    "   \n",
    "train_generator = train_datagen.flow_from_directory(  \n",
    "      train_data_dir,  \n",
    "      target_size=(img_width, img_height),  \n",
    "      batch_size=batch_size,  \n",
    "      class_mode=None,  \n",
    "      shuffle=False)  \n",
    "   \n",
    "nb_train_samples = len(train_generator.filenames)  \n",
    "num_classes = len(train_generator.class_indices)  \n",
    "   \n",
    "predict_size_train = int(math.ceil(nb_train_samples / batch_size))  \n",
    "   \n",
    "bottleneck_features_train = vgg19.predict_generator(train_generator, predict_size_train)  \n",
    "   \n",
    "np.save('/content/gdrive/My Drive/Gdrive/bottleneck_features_train.npy', bottleneck_features_train)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)\n",
    "\n",
    "#Pre-train Validation cuy\n",
    "start = datetime.datetime.now()\n",
    "generator = datagen.flow_from_directory(  \n",
    "      validation_data_dir,  \n",
    "      target_size=(img_width, img_height),  \n",
    "      batch_size=batch_size,  \n",
    "      class_mode=None,  \n",
    "      shuffle=False)  \n",
    "   \n",
    "nb_validation_samples = len(generator.filenames)  \n",
    "num_classes = len(generator.class_indices)  \n",
    "\n",
    "predict_size_validation = int(math.ceil(nb_validation_samples / batch_size))  \n",
    "   \n",
    "bottleneck_features_validation = vgg19.predict_generator(  \n",
    "      generator, predict_size_validation)  \n",
    "   \n",
    "np.save('/content/gdrive/My Drive/Gdrive/bottleneck_features_validation.npy', bottleneck_features_validation) \n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)\n",
    "\n",
    "#Pre train Testing\n",
    "start = datetime.datetime.now()\n",
    "generator = datagen.flow_from_directory(  \n",
    "      test_data_dir,  \n",
    "      target_size=(img_width, img_height),  \n",
    "      batch_size=batch_size,  \n",
    "      class_mode=None,  \n",
    "      shuffle=False)  \n",
    "   \n",
    "nb_test_samples = len(generator.filenames)  \n",
    "   \n",
    "predict_size_test = int(math.ceil(nb_test_samples / batch_size))  \n",
    "   \n",
    "bottleneck_features_test = vgg19.predict_generator(  \n",
    "      generator, predict_size_test)  \n",
    "   \n",
    "np.save('/content/gdrive/My Drive/Gdrive/bottleneck_features_test.npy', bottleneck_features_test) \n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "N2jWEeTVgWyq",
    "outputId": "3182d468-d32b-40ee-a70c-c238044ac37d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-64834291de0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m              \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m              \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Confusion matrix'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m              cmap=plt.cm.Blues):\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#TRAINING dan Testing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#TRAINING\n",
    "#hasil= []\n",
    "#vgg19 = applications.vgg19(include_top=False, weights='imagenet') \n",
    "def read_image(file_path):\n",
    "    print(\"[INFO] loading and preprocessing image...\")  \n",
    "    image = load_img(file_path, target_size=(224, 224))  \n",
    "    image = img_to_array(image)  \n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image /= 255.  \n",
    "    return image\n",
    "\n",
    "#To get better visual of the confusion matrix:\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "             normalize=False,\n",
    "             title='Confusion matrix',\n",
    "             cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"confusion matrix yang dinormalisasi \\n\\n\\n\")\n",
    "    else:\n",
    "        print('Confusion matrix tanpa normalisasi \\n\\n\\n')\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "img_width, img_height = 224, 224  \n",
    "top_model_weights_path = 'bottleneck_fc_model.h5' \n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1. / 255) \n",
    " \n",
    "#training data\n",
    "generator_top = datagen.flow_from_directory(  \n",
    "         train_data_dir,  \n",
    "         target_size=(img_width, img_height),  \n",
    "         batch_size=batch_size,  \n",
    "         class_mode='categorical',  \n",
    "         shuffle=False)  \n",
    "   \n",
    "nb_train_samples = len(generator_top.filenames)  \n",
    "num_classes = len(generator_top.class_indices)  \n",
    "   \n",
    "train_data = np.load('/content/gdrive/My Drive/Gdrive/bottleneck_features_train.npy')  \n",
    "   \n",
    "train_labels = generator_top.classes  \n",
    "   \n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes) \n",
    "\n",
    "#testing data\n",
    "generator_top = datagen.flow_from_directory(  \n",
    "         test_data_dir,  \n",
    "         target_size=(img_width, img_height),  \n",
    "         batch_size=batch_size,  \n",
    "         class_mode=None,  \n",
    "         shuffle=False)  \n",
    "   \n",
    "nb_test_samples = len(generator_top.filenames)  \n",
    "   \n",
    "test_data = np.load('/content/gdrive/My Drive/Gdrive/bottleneck_features_test.npy')  \n",
    "   \n",
    "\n",
    "test_labels = generator_top.classes  \n",
    "test_labels = to_categorical(test_labels, num_classes=num_classes)\n",
    "\n",
    "#Lanjut training \n",
    "start = datetime.datetime.now()\n",
    "model = Sequential()  \n",
    "model.add(Flatten(input_shape=train_data.shape[1:]))  \n",
    "#model.add(Dense(100, activation=LeakyReLU(alpha=0.3)))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.3))\n",
    "model.add(Dropout(0.5))  \n",
    "#model.add(Dense(50, activation=LeakyReLU(alpha=0.3)))\n",
    "model.add(Dense(50))\n",
    "model.add(LeakyReLU(alpha=0.3))  \n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=lr),\n",
    "              metrics=['acc'])  \n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint('/content/gdrive/My Drive/models/vgg19/best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# history = model.fit(train_data, train_labels,  \n",
    "#       epochs=20,\n",
    "#       batch_size=batch_size,  \n",
    "#       validation_data=(test_data, test_labels))\n",
    "\n",
    "\n",
    "history = model.fit(train_data, train_labels,  \n",
    "      epochs=100,\n",
    "      batch_size=batch_size,  \n",
    "      validation_data=(test_data, test_labels),\n",
    "      verbose=0,\n",
    "      callbacks=[es, mc])    \n",
    "\n",
    "model.save_weights(top_model_weights_path)  \n",
    "\n",
    "(eval_loss, eval_accuracy) = model.evaluate(  \n",
    " test_data, test_labels, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))  \n",
    "print(\"[INFO] Loss: {}\".format(eval_loss))  \n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)\n",
    "\n",
    "#Model summary\n",
    "model.summary()\n",
    "\n",
    "#Graphing our training and validation\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plotacc = plt.figure(1)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plotacc.show()\n",
    "plt.savefig(\"/content/gdrive/My Drive/gambar/vgg19/train/Acc_vgg19_{}Batch_{}E_Opt={}_lr={}.jpg\".format(batch_size, len(acc),opt,lr))\n",
    "\n",
    "\n",
    "\n",
    "plotloss = plt.figure(2)\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plotloss.show()\n",
    "plt.savefig(\"/content/gdrive/My Drive/gambar/vgg19/train/Loss_vgg19_{}Batch_{}E_Opt={}_lr={}.jpg\".format(batch_size, len(acc),opt,lr))\n",
    "\n",
    "Evaluasi=model.evaluate(test_data, test_labels)\n",
    "print(Evaluasi)\n",
    "\n",
    "\n",
    "print('test data', test_data)\n",
    "preds = np.round(model.predict(test_data),0) \n",
    "score = model.predict(test_data)\n",
    "#to fit them into classification metrics and confusion metrics, some additional modificaitions are required\n",
    "print('rounded test_labels', preds)\n",
    "#Model di save \n",
    "model.save('/content/gdrive/My Drive/models/vgg19/model_{}E.h5'.format(len(acc)))\n",
    "\n",
    "view = ['Vhigh','High','Low','Vlow','Medium']\n",
    "classification_metrics = metrics.classification_report(test_labels, preds, target_names=view )\n",
    "print(classification_metrics)\n",
    "\n",
    "#Since our data is in dummy format we put the numpy array into a dataframe and call idxmax axis=1 to return the column\n",
    "# label of the maximum value thus creating a categorical variable\n",
    "#Basically, flipping a dummy variable back to it's categorical variable\n",
    "categorical_test_labels = pd.DataFrame(test_labels).idxmax(axis=1)\n",
    "categorical_preds = pd.DataFrame(preds).idxmax(axis=1)\n",
    "confusion_matrix= confusion_matrix(categorical_test_labels, categorical_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "id": "3M5wgCEq2SkR",
    "outputId": "1eecde4a-b414-4ca7-e0d3-ac7e8a700378"
   },
 
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P1sFvYzJg9Kt",
    "outputId": "5500635d-e4af-41a2-edcb-096c7dbb3469"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b64d664f5bf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m              \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m              \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Confusion matrix'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m              cmap=plt.cm.Blues):\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ]
    " \n",
    "def read_image(file_path):\n",
    "    print(\"[INFO] loading and preprocessing image...\")  \n",
    "    image = load_img(file_path, target_size=(224, 224))  \n",
    "    image = img_to_array(image)  \n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image /= 255.  \n",
    "    return image\n",
    "\n",
    "
    "\n",
    "\n",
    "def predict(path):\n",
    "    view = ['Vhigh','High','Low','Vlow','Medium']\n",
    "    images = read_image(path)\n",
    "#    time.sleep(.5)\n",
    "    bt_prediction = vgg19.predict(images)  \n",
    "    preds = model.predict_proba(bt_prediction)\n",
    "#    for idx, view, x in zip(range(0,6), view , preds[0]):\n",
    "#        print(\"ID: {}, Label: {} {}%\".format(idx, view, round(x*100,2) ))\n",
    "#    print('Final Decision:')\n",
    "#    time.sleep(.5)\n",
    "#    for x in range(3):\n",
    "#        print('.'*(x+1))\n",
    "#        time.sleep(.2)\n",
    "    class_predicted = model.predict_classes(bt_prediction)\n",
    "    class_dictionary = generator_top.class_indices  \n",
    "    inv_map = {v: k for k, v in class_dictionary.items()}  \n",
    "#    print(\"ID: {}, Label: {}\".format(class_predicted[0], inv_map[class_predicted[0]]))  \n",
    "    return load_img(path)\n",
    "\n",
    "img_width, img_height = 224, 224  \n",
    "top_model_weights_path = 'bottleneck_fc_model.h5' \n",
    "\n",
    "  \n",
    "datagen = ImageDataGenerator(rescale=1. / 255) \n",
    "\n",
    "#training data\n",
    "generator_top = datagen.flow_from_directory(  \n",
    "         train_data_dir,  \n",
    "         target_size=(img_width, img_height),  \n",
    "         batch_size=batch_size,  \n",
    "         class_mode='categorical',  \n",
    "         shuffle=False)  \n",
    "   \n",
    "nb_train_samples = len(generator_top.filenames)  \n",
    "num_classes = len(generator_top.class_indices)  \n",
    "   \n",
    "train_data = np.load('/content/gdrive/My Drive/Gdrive/bottleneck_features_train.npy')  \n",
    "\n",
    "train_labels = generator_top.classes  \n",
    "   \n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes) \n",
    "\n",
    "#validation data\n",
    "generator_top = datagen.flow_from_directory(  \n",
    "         validation_data_dir,  \n",
    "         target_size=(img_width, img_height),  \n",
    "         batch_size=batch_size,  \n",
    "         class_mode=None,  \n",
    "         shuffle=False)  \n",
    "   \n",
    "nb_validation_samples = len(generator_top.filenames)  \n",
    "   \n",
    "validation_data = np.load('/content/gdrive/My Drive/Gdrive/bottleneck_features_validation.npy')  \n",
    "   \n",
    "\n",
    "validation_labels = generator_top.classes  \n",
    "validation_labels = to_categorical(validation_labels, num_classes=num_classes)\n",
    "\n",
    "#testing data\n",
    "generator_top = datagen.flow_from_directory(  \n",
    "         test_data_dir,  \n",
    "         target_size=(img_width, img_height),  \n",
    "         batch_size=batch_size,  \n",
    "         class_mode=None,  \n",
    "         shuffle=False)  \n",
    "   \n",
    "nb_test_samples = len(generator_top.filenames)  \n",
    "   \n",
    "test_data = np.load('/content/gdrive/My Drive/Gdrive/bottleneck_features_test.npy')  \n",
    "   \n",
    "\n",
    "test_labels = generator_top.classes  \n",
    "test_labels = to_categorical(test_labels, num_classes=num_classes)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "#model = tf.keras.models.load_model('/content/gdrive/My Drive/CODE/models/vgg19/best_model.h5')\n",
    "model = tf.keras.models.load_model('/content/gdrive/My Drive/models/vgg19/best_model.h5')\n",
    "model.summary()\n",
    "\n",
    "model.evaluate(validation_data, validation_labels)\n",
    "\n",
    "print('validation data', validation_data)\n",
    "preds = np.round(model.predict(validation_data),0) \n",
    "score = model.predict(validation_data)\n",
    "#to fit them into classification metrics and confusion metrics, some additional modificaitions are required\n",
    "print('rounded validation_labels', preds)\n",
    "\n",
    "view = ['Vhigh','High','Low','Vlow','Medium']\n",
    "classification_metrics = metrics.classification_report(validation_labels, preds, target_names=view )\n",
    "print(classification_metrics)\n",
    "\n",
    "categorical_validation_labels = pd.DataFrame(validation_labels).idxmax(axis=1)\n",
    "categorical_preds = pd.DataFrame(preds).idxmax(axis=1)\n",
    "confusion_matrix= confusion_matrix(categorical_validation_labels, categorical_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "4-mtOsMAhZVg",
    "outputId": "52ee1eb3-28dd-488a-f937-535f0fc1c1e6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-af7080a09b09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplotcmnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Halal'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Haram'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'meragukan'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/content/gdrive/My Drive/gambar/vgg19/test/CMNN_vgg19_{}Batch_{}E_Opt={}_lr={}.jpg\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# plotcmn = plt.figure(6)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
  
